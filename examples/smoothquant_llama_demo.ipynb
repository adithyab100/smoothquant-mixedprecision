{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyab100/smoothquant-mixedprecision/blob/main/examples/smoothquant_llama_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v23yEhSbiEYl"
      },
      "source": [
        "# SmoothQuant on Llama 2 7B\n",
        "\n",
        "In this notebook, we use Llama-2-7B model to demonstrate SmoothQuant can use 8-bit for both weights and activations to achieve the similar perplexity as FP16 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_h55-apiEYm"
      },
      "source": [
        "In order to run this notebook, you need to install the following packages:\n",
        "\n",
        "- smoothquant\n",
        "- PyTorch\n",
        "- Transformers\n",
        "- Accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adithyab100/smoothquant-mixedprecision.git"
      ],
      "metadata": {
        "id": "3i72H4mvjDwL",
        "outputId": "9daa7a7a-6aa7-4f2a-b2cc-4a748941fae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'smoothquant-mixedprecision'...\n",
            "remote: Enumerating objects: 346, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/187)\u001b[K\rremote: Counting objects:   1% (2/187)\u001b[K\rremote: Counting objects:   2% (4/187)\u001b[K\rremote: Counting objects:   3% (6/187)\u001b[K\rremote: Counting objects:   4% (8/187)\u001b[K\rremote: Counting objects:   5% (10/187)\u001b[K\rremote: Counting objects:   6% (12/187)\u001b[K\rremote: Counting objects:   7% (14/187)\u001b[K\rremote: Counting objects:   8% (15/187)\u001b[K\rremote: Counting objects:   9% (17/187)\u001b[K\rremote: Counting objects:  10% (19/187)\u001b[K\rremote: Counting objects:  11% (21/187)\u001b[K\rremote: Counting objects:  12% (23/187)\u001b[K\rremote: Counting objects:  13% (25/187)\u001b[K\rremote: Counting objects:  14% (27/187)\u001b[K\rremote: Counting objects:  15% (29/187)\u001b[K\rremote: Counting objects:  16% (30/187)\u001b[K\rremote: Counting objects:  17% (32/187)\u001b[K\rremote: Counting objects:  18% (34/187)\u001b[K\rremote: Counting objects:  19% (36/187)\u001b[K\rremote: Counting objects:  20% (38/187)\u001b[K\rremote: Counting objects:  21% (40/187)\u001b[K\rremote: Counting objects:  22% (42/187)\u001b[K\rremote: Counting objects:  23% (44/187)\u001b[K\rremote: Counting objects:  24% (45/187)\u001b[K\rremote: Counting objects:  25% (47/187)\u001b[K\rremote: Counting objects:  26% (49/187)\u001b[K\rremote: Counting objects:  27% (51/187)\u001b[K\rremote: Counting objects:  28% (53/187)\u001b[K\rremote: Counting objects:  29% (55/187)\u001b[K\rremote: Counting objects:  30% (57/187)\u001b[K\rremote: Counting objects:  31% (58/187)\u001b[K\rremote: Counting objects:  32% (60/187)\u001b[K\rremote: Counting objects:  33% (62/187)\u001b[K\rremote: Counting objects:  34% (64/187)\u001b[K\rremote: Counting objects:  35% (66/187)\u001b[K\rremote: Counting objects:  36% (68/187)\u001b[K\rremote: Counting objects:  37% (70/187)\u001b[K\rremote: Counting objects:  38% (72/187)\u001b[K\rremote: Counting objects:  39% (73/187)\u001b[K\rremote: Counting objects:  40% (75/187)\u001b[K\rremote: Counting objects:  41% (77/187)\u001b[K\rremote: Counting objects:  42% (79/187)\u001b[K\rremote: Counting objects:  43% (81/187)\u001b[K\rremote: Counting objects:  44% (83/187)\u001b[K\rremote: Counting objects:  45% (85/187)\u001b[K\rremote: Counting objects:  46% (87/187)\u001b[K\rremote: Counting objects:  47% (88/187)\u001b[K\rremote: Counting objects:  48% (90/187)\u001b[K\rremote: Counting objects:  49% (92/187)\u001b[K\rremote: Counting objects:  50% (94/187)\u001b[K\rremote: Counting objects:  51% (96/187)\u001b[K\rremote: Counting objects:  52% (98/187)\u001b[K\rremote: Counting objects:  53% (100/187)\u001b[K\rremote: Counting objects:  54% (101/187)\u001b[K\rremote: Counting objects:  55% (103/187)\u001b[K\rremote: Counting objects:  56% (105/187)\u001b[K\rremote: Counting objects:  57% (107/187)\u001b[K\rremote: Counting objects:  58% (109/187)\u001b[K\rremote: Counting objects:  59% (111/187)\u001b[K\rremote: Counting objects:  60% (113/187)\u001b[K\rremote: Counting objects:  61% (115/187)\u001b[K\rremote: Counting objects:  62% (116/187)\u001b[K\rremote: Counting objects:  63% (118/187)\u001b[K\rremote: Counting objects:  64% (120/187)\u001b[K\rremote: Counting objects:  65% (122/187)\u001b[K\rremote: Counting objects:  66% (124/187)\u001b[K\rremote: Counting objects:  67% (126/187)\u001b[K\rremote: Counting objects:  68% (128/187)\u001b[K\rremote: Counting objects:  69% (130/187)\u001b[K\rremote: Counting objects:  70% (131/187)\u001b[K\rremote: Counting objects:  71% (133/187)\u001b[K\rremote: Counting objects:  72% (135/187)\u001b[K\rremote: Counting objects:  73% (137/187)\u001b[K\rremote: Counting objects:  74% (139/187)\u001b[K\rremote: Counting objects:  75% (141/187)\u001b[K\rremote: Counting objects:  76% (143/187)\u001b[K\rremote: Counting objects:  77% (144/187)\u001b[K\rremote: Counting objects:  78% (146/187)\u001b[K\rremote: Counting objects:  79% (148/187)\u001b[K\rremote: Counting objects:  80% (150/187)\u001b[K\rremote: Counting objects:  81% (152/187)\u001b[K\rremote: Counting objects:  82% (154/187)\u001b[K\rremote: Counting objects:  83% (156/187)\u001b[K\rremote: Counting objects:  84% (158/187)\u001b[K\rremote: Counting objects:  85% (159/187)\u001b[K\rremote: Counting objects:  86% (161/187)\u001b[K\rremote: Counting objects:  87% (163/187)\u001b[K\rremote: Counting objects:  88% (165/187)\u001b[K\rremote: Counting objects:  89% (167/187)\u001b[K\rremote: Counting objects:  90% (169/187)\u001b[K\rremote: Counting objects:  91% (171/187)\u001b[K\rremote: Counting objects:  92% (173/187)\u001b[K\rremote: Counting objects:  93% (174/187)\u001b[K\rremote: Counting objects:  94% (176/187)\u001b[K\rremote: Counting objects:  95% (178/187)\u001b[K\rremote: Counting objects:  96% (180/187)\u001b[K\rremote: Counting objects:  97% (182/187)\u001b[K\rremote: Counting objects:  98% (184/187)\u001b[K\rremote: Counting objects:  99% (186/187)\u001b[K\rremote: Counting objects: 100% (187/187)\u001b[K\rremote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 346 (delta 128), reused 113 (delta 96), pack-reused 159 (from 1)\u001b[K\n",
            "Receiving objects: 100% (346/346), 6.80 MiB | 24.51 MiB/s, done.\n",
            "Resolving deltas: 100% (198/198), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D5ASu0M8iEYm",
        "outputId": "4448f4d2-4b95-4996-de93-61c45c40db67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'smoothquant'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-606378351892>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmoothquant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmooth_lm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmoothquant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_quant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantize_llama_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smoothquant'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import (\n",
        "    LlamaAttention,\n",
        "    LlamaDecoderLayer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaMLP,\n",
        ")\n",
        "from transformers import LlamaTokenizer\n",
        "from smoothquant.smooth import smooth_lm\n",
        "from smoothquant.fake_quant import quantize_llama_like\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWDvqPBliEYm"
      },
      "source": [
        "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 40 examples in the test set of the Wikitext-2 dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV5sxZs8iEYm"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "        self.dataset = tokenizer(\n",
        "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
        "        ).input_ids.to(device)\n",
        "\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, model):\n",
        "        model.eval()\n",
        "        nlls = []\n",
        "        for i in tqdm.tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
        "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
        "            with torch.no_grad():\n",
        "                lm_logits = model(batch).logits\n",
        "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
        "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
        "            )\n",
        "            neg_log_likelihood = loss.float() * 2048\n",
        "            nlls.append(neg_log_likelihood)\n",
        "\n",
        "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP75YqF9iEYn",
        "outputId": "dfe026fc-b0dc-42cb-b1c7-c0e5724f4807"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/NFS/home/guangxuan/anaconda3/envs/ellm/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViOypjmmiEYn"
      },
      "source": [
        "## FP16 Model Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5V66zhAiEYn"
      },
      "source": [
        "Let's first check the performance of the original FP16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4QVy1PUiEYn",
        "outputId": "db80a843-7a55-4ab3-d7db-d558794a33df",
        "colab": {
          "referenced_widgets": [
            "27d329da8d934f258323e1977002cebd"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27d329da8d934f258323e1977002cebd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_fp16 = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz9SOnENiEYn",
        "outputId": "4ac9140d-a005-4d36-deb4-06dbe54c519c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating...: 100%|██████████| 40/40 [00:09<00:00,  4.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model (fp16) perplexity: 5.822948932647705\n"
          ]
        }
      ],
      "source": [
        "ppl_fp16 = evaluator.evaluate(model_fp16)\n",
        "print(f\"Original model (fp16) perplexity: {ppl_fp16}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxmMRgUTiEYn"
      },
      "source": [
        "We then quantize the model to W8A8 and check the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noaI-0_HiEYn"
      },
      "source": [
        "## Naive W8A8 Quantized Model Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl0ILrWHiEYn",
        "outputId": "13f46cac-ce9c-47e7-be01-844c03bf7d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model_w8a8 = quantize_llama_like(model_fp16)\n",
        "print(model_w8a8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKhz6Bm4iEYn",
        "outputId": "bbd51ed2-bce1-4dc0-f1ef-573b19d57d73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating...: 100%|██████████| 40/40 [00:07<00:00,  5.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive W8A8 quantized model perplexity: 5.931240558624268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ppl_w8a8 = evaluator.evaluate(model_w8a8)\n",
        "print(f\"Naive W8A8 quantized model perplexity: {ppl_w8a8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IbHJoeMiEYo"
      },
      "source": [
        "We can see there is a perplexity increase. We then use SmoothQuant to quantize the model and check the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6dXTdUSiEYo"
      },
      "source": [
        "## SmoothQuant W8A8 Quantized Model Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjjyubz9iEYo",
        "outputId": "37ecd04a-bd2c-4b6b-eea1-cd21c9e403b8",
        "colab": {
          "referenced_widgets": [
            "321b7c77ef984e6f82c74c8b6b16e558"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "321b7c77ef984e6f82c74c8b6b16e558",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
        "smooth_lm(model, act_scales, 0.85)\n",
        "model_smoothquant_w8a8 = quantize_llama_like(model)\n",
        "print(model_smoothquant_w8a8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdO2Y7U6iEYo"
      },
      "source": [
        "We can see the smoothed model has a lower perplexity which is close to the FP16 model's. This is because SmoothQuant smooths the outliers in activations and balances the quantization difficulty of activations and weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCcS-GAqiEYo",
        "outputId": "028728b7-afa4-4928-86aa-c054556d82b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating...: 100%|██████████| 40/40 [00:08<00:00,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SmoothQuant W8A8 quantized model perplexity: 5.85634183883667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ppl_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
        "print(f\"SmoothQuant W8A8 quantized model perplexity: {ppl_smoothquant_w8a8}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}