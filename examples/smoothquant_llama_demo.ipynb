{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyab100/smoothquant-mixedprecision/blob/main/examples/smoothquant_llama_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v23yEhSbiEYl"
      },
      "source": [
        "# SmoothQuant on Llama 2 7B\n",
        "\n",
        "In this notebook, we use Llama-2-7B model to demonstrate SmoothQuant can use 8-bit for both weights and activations to achieve the similar perplexity as FP16 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_h55-apiEYm"
      },
      "source": [
        "In order to run this notebook, you need to install the following packages:\n",
        "\n",
        "- smoothquant\n",
        "- PyTorch\n",
        "- Transformers\n",
        "- Accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adithyab100/smoothquant-mixedprecision.git\n",
        "%cd smoothquant-mixedprecision"
      ],
      "metadata": {
        "id": "3i72H4mvjDwL",
        "outputId": "0015f483-603e-4c59-c504-7f099b594ded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'smoothquant-mixedprecision'...\n",
            "remote: Enumerating objects: 358, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 358 (delta 135), reused 117 (delta 97), pack-reused 159 (from 1)\u001b[K\n",
            "Receiving objects: 100% (358/358), 6.82 MiB | 21.10 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n",
            "/content/smoothquant-mixedprecision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install transformers==4.36.0 accelerate datasets zstandard\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FhW44sIrpR-F",
        "outputId": "6ab07dca-f031-425a-bc2f-3ce199716e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m459.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2024.8.30)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
            "Collecting transformers==4.36.0\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting zstandard\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.12.1+cu113)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard, xxhash, fsspec, dill, multiprocess, tokenizers, transformers, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.2\n",
            "    Uninstalling transformers-4.46.2:\n",
            "      Successfully uninstalled transformers-4.46.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 tokenizers-0.15.2 transformers-4.36.0 xxhash-3.5.0 zstandard-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l-GrYsRvqIow",
        "outputId": "d095914c-675c-4ee6-a1bb-fb43d89ae528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating smoothquant.egg-info\n",
            "writing smoothquant.egg-info/PKG-INFO\n",
            "writing dependency_links to smoothquant.egg-info/dependency_links.txt\n",
            "writing top-level names to smoothquant.egg-info/top_level.txt\n",
            "writing manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "reading manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/lib/smoothquant\n",
            "copying smoothquant/ppl_eval.py -> build/lib/smoothquant\n",
            "copying smoothquant/calibration.py -> build/lib/smoothquant\n",
            "copying smoothquant/fake_quant.py -> build/lib/smoothquant\n",
            "copying smoothquant/smooth.py -> build/lib/smoothquant\n",
            "copying smoothquant/__init__.py -> build/lib/smoothquant\n",
            "copying smoothquant/opt.py -> build/lib/smoothquant\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/ppl_eval.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/calibration.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/fake_quant.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/smooth.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/__init__.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/opt.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/ppl_eval.py to ppl_eval.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/calibration.py to calibration.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/fake_quant.py to fake_quant.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/smooth.py to smooth.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/opt.py to opt.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/smoothquant-0.0.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing smoothquant-0.0.0-py3.10.egg\n",
            "Copying smoothquant-0.0.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding smoothquant 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/smoothquant-0.0.0-py3.10.egg\n",
            "Processing dependencies for smoothquant==0.0.0\n",
            "Finished processing dependencies for smoothquant==0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D5ASu0M8iEYm"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import (\n",
        "    LlamaAttention,\n",
        "    LlamaDecoderLayer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaMLP,\n",
        ")\n",
        "from transformers import LlamaTokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from smoothquant.smooth import smooth_lm\n",
        "from smoothquant.fake_quant import quantize_llama_like, quantize_opt\n",
        "import tqdm\n",
        "import gc\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWDvqPBliEYm"
      },
      "source": [
        "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 40 examples in the test set of the Wikitext-2 dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZV5sxZs8iEYm"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "        self.dataset = tokenizer(\n",
        "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
        "        ).input_ids.to(device)\n",
        "\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, model):\n",
        "        model.eval()\n",
        "        nlls = []\n",
        "        for i in tqdm.tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
        "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
        "            with torch.no_grad():\n",
        "                lm_logits = model(batch).logits\n",
        "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
        "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
        "            )\n",
        "            neg_log_likelihood = loss.float() * 2048\n",
        "            nlls.append(neg_log_likelihood)\n",
        "\n",
        "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, tokenizer):\n",
        "    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
        "\n",
        "    testenc = testenc.input_ids.to(model.device)\n",
        "    nsamples = 40\n",
        "    model = model.eval()\n",
        "\n",
        "    nlls = []\n",
        "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
        "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n",
        "        with torch.no_grad():\n",
        "            lm_logits = model(batch).logits\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
        "        shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        neg_log_likelihood = loss.float() * 2048\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))"
      ],
      "metadata": {
        "id": "rpOgMwI3myd1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n",
        "\n",
        "    if group_size != -1:\n",
        "        data_width += (16 + 4) / group_size\n",
        "\n",
        "    num_elements = 0\n",
        "    for param in model.parameters():\n",
        "        num_elements += param.numel()\n",
        "    return num_elements * data_width\n",
        "\n",
        "Byte = 8\n",
        "KiB = 1024 * Byte\n",
        "MiB = 1024 * KiB\n",
        "GiB = 1024 * MiB"
      ],
      "metadata": {
        "id": "kIIuE_TcnDm_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lP75YqF9iEYn",
        "outputId": "8d305ab4-3cbe-472f-e953-e56a4098a3b9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 14.75 GiB total capacity; 13.46 GiB already allocated; 229.06 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-1a84c225315a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nmodel perplexity: {model_perplexity:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-81cc4e794d32>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestenc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mshift_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestenc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m   1144\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    907\u001b[0m                 )\n\u001b[1;32m    908\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    910\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;34mf\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 )\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             attn_weights = torch.max(\n\u001b[1;32m    234\u001b[0m                 \u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 14.75 GiB total capacity; 13.46 GiB already allocated; 229.06 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "model_path = \"facebook/opt-1.3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "\n",
        "# Evaluate the model\n",
        "model_perplexity = evaluate(model, tokenizer)\n",
        "model_size = get_model_size(model, data_width=32, group_size=128)\n",
        "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
        "print(f\"model size: {model_size/MiB:.2f} MiB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_w4a4 = quantize_opt(model)\n",
        "print(model_w4a4)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L1qYO_ZVrOs-",
        "outputId": "c1b0bcd5-3c93-4c23-86be-74d59bda9485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPTForCausalLM(\n",
            "  (model): OPTModel(\n",
            "    (decoder): OPTDecoder(\n",
            "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
            "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
            "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (8): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (9): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (10): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (11): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (12): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (13): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (14): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (15): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (16): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (17): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (18): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (19): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (20): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (21): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (22): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (23): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ppl_w4a4 = evaluate(model_w4a4,tokenizer)\n",
        "print(f\"Naive W4A4 quantized model perplexity: {ppl_w4a4}\")\n",
        "model_size = get_model_size(model_w4a4, data_width=2, group_size=128)\n"
      ],
      "metadata": {
        "id": "KIksVu6yrU7u",
        "outputId": "96a71751-923c-4727-ab32-149432bca5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating...: 100%|██████████| 40/40 [01:23<00:00,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive W4A4 quantized model perplexity: 32997.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nmodel perplexity: {ppl_w4a4:.2f}\")\n",
        "print(f\"model size: {model_size/MiB:.2f} MiB\")"
      ],
      "metadata": {
        "id": "O-d2s0D0s2q2",
        "outputId": "33daab2d-5eff-4c78-ef65-782bee4842fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model perplexity: 32997.12\n",
            "model size: 338.21 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
        "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    samples = []\n",
        "    n_run = 0\n",
        "    for data in dataset:\n",
        "        line = data[\"text\"]\n",
        "        line = line.strip()\n",
        "        line_encoded = tokenizer.encode(line)\n",
        "        if len(line_encoded) > block_size:\n",
        "            continue\n",
        "        sample = torch.tensor([line_encoded])\n",
        "        if sample.numel() == 0:\n",
        "            continue\n",
        "        samples.append(sample)\n",
        "        n_run += 1\n",
        "        if n_run == n_samples:\n",
        "            break\n",
        "\n",
        "    # now concatenate all samples and split according to block size\n",
        "    cat_samples = torch.cat(samples, dim=1)\n",
        "    n_split = cat_samples.shape[1] // block_size\n",
        "    print(f\" * Split into {n_split} blocks\")\n",
        "    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_calib_feat(model, tokenizer):\n",
        "    input_dict = dict()\n",
        "    def stat_input_max_hook(m, x, y, name):\n",
        "        if isinstance(x, tuple):\n",
        "            x = x[0]\n",
        "        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()\n",
        "        if name not in input_dict:\n",
        "            input_dict[name] = [x_max]\n",
        "        else:\n",
        "            input_dict[name] += [x_max]\n",
        "\n",
        "    hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            hooks.append(\n",
        "                m.register_forward_hook(\n",
        "                    partial(stat_input_max_hook, name=name)))\n",
        "\n",
        "    print(\"Collecting activation scales...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    samples = get_calib_dataset(tokenizer)\n",
        "    pbar = tqdm.tqdm(samples)\n",
        "    for input_ids in pbar:\n",
        "        input_ids = input_ids.to(device)\n",
        "        model(input_ids)\n",
        "\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "    return input_dict"
      ],
      "metadata": {
        "id": "JKtkmiystdBe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", force_download= True)"
      ],
      "metadata": {
        "id": "xDHYjRrPtd_B",
        "outputId": "98f4cd50-734c-44bf-bc27-369f8637f269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201,
          "referenced_widgets": [
            "dc13bd42fa7148ddb4151cb79fde159d",
            "d2bfa6e8f0ef4bc0bebdbd4e07c6ffda",
            "68cb7f43a3fb4bc78da3f1eb10fa71f4",
            "05889ac04de0418e8da134931c68086c",
            "c78ffba17b84469ba3671d2690287bfd",
            "273cb8d24d3b4394a77baff8efc718e1",
            "9b837dc2e80f414581f7f7255b5ba9fe",
            "9cd145bf78cc43059cc0d49e92237b18",
            "bb664b00e46b4c159dd6a3d87ef686a4",
            "da40483996524c0da676d5902af5962e",
            "3267cc14e67e40119f3c5e526c432b63",
            "cab584a76d874ce7949b6e9eb44dbbaa",
            "3962b1a744934923a60878dc1b45ec60",
            "e0f1e0bcc2bd410aaabeac56a596101f",
            "419c6f11ddc14c0f98691afb93fdc1b5",
            "7801d8220f5147aa99f50d4933934c40",
            "ec66d48a16134e5785a0c2cc1490c560",
            "4b88a0f0d4bc40a3b1886d001e32a8f1",
            "38405eaa02984c2687a43011c922924f",
            "37cddfde58b84add9f6a2fafe399eb5d",
            "886f1b48137e44a6aaabdf687e00cd17",
            "216a8226ddc340e38a4d44aca034a172",
            "a68d482395ea47c0a1245432aa2ddb10",
            "963d34f7439f42a6be5449a3472744d1",
            "e88335905554416b93dd90ac1e0f93fb",
            "e9ea360c13f14354b534991d466a1f55",
            "f52929d30df4490683f851d6125ba6cb",
            "dfe43cc23bfb4f0c98ca52183ad260c2",
            "8ea3f3b1370e442ea12d2a7118799a58",
            "8327272df32840aaabd4fec479a17a1f",
            "6f63aff5c4a74e00bfc77f208e7dee2c",
            "0b1e311211b440c0a0de55d5c06199bf",
            "6ba4638f89474a55885474c8d6359f17",
            "3c50dfed7a5047ef83e2300c88aed99b",
            "8684163d63eb45e7ad956455a6ad2c8a",
            "fcbe278955544f3d9aa04d08d1cf8d88",
            "843d56b8232440dcb7e6e8a3b75f8fca",
            "4edfd42be5a3442b8aafbdb654f0a99a",
            "1cd83e4d75ea4ae2a534f753daef63ff",
            "d179b0f76b3c462885420195290b4eb6",
            "97d870e883e043338c3062aae764a65e",
            "3e26c6b5eaec45e6989400d907c82535",
            "923de711ca6e4329857a0ac7d14cf67b",
            "b46a3e50d21b4e21ae90b61a46fece84"
          ]
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc13bd42fa7148ddb4151cb79fde159d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cab584a76d874ce7949b6e9eb44dbbaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a68d482395ea47c0a1245432aa2ddb10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c50dfed7a5047ef83e2300c88aed99b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_feat = get_calib_feat(model, tokenizer)\n"
      ],
      "metadata": {
        "id": "mUgMZVgML87Q",
        "outputId": "92af2521-684c-4285-de89-da78c829c5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting activation scales...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Split into 127 blocks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127/127 [00:50<00:00,  2.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_w4a4 = quantize_opt(model, input_feat = input_feat, salient_prop = 0.01)\n",
        "print(model_w4a4)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qYqxsuXpIO7S",
        "outputId": "d7eacddc-ffda-44c6-ff5e-a27d6b23b332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPTForCausalLM(\n",
            "  (model): OPTModel(\n",
            "    (decoder): OPTDecoder(\n",
            "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
            "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
            "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1500, 1910, 1128,  306, 1767, 1157,   25, 2011,    7, 1213, 1219, 1950,\n",
            "                    1528,   24, 1202, 1359, 1285, 1455, 2006,  653]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1500, 1910, 1128,  306, 1767, 1157,   25, 2011,    7, 1213, 1219, 1950,\n",
            "                    1528,   24, 1202, 1359, 1285, 1455, 2006,  653]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1500, 1910, 1128,  306, 1767, 1157,   25, 2011,    7, 1213, 1219, 1950,\n",
            "                    1528,   24, 1202, 1359, 1285, 1455, 2006,  653]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1675, 1142, 1691, 1706, 1708, 1133, 1546, 1028, 1185, 1670, 1195, 1696,\n",
            "                    1563, 1683, 1672, 1694,  859, 1108, 1921,  155]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359,  306, 1767, 1500, 1177, 1910,  591, 1128, 1157,  807,  654,   25,\n",
            "                  1557,   24,  572, 1213, 2011,  236, 1672, 1285]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([4397, 3398, 5776, 1194, 4511,  293, 6079, 1471, 2647, 6366, 1578, 1869,\n",
            "                  8078, 7141, 5818, 1506, 3111, 3710, 7548, 4346, 4806, 2604, 2383,  785,\n",
            "                  2084, 5759,  136,  753, 7397, 4898, 5865, 3272, 4209, 7960,  754, 7828,\n",
            "                  3206, 2897,  787, 4850,  225, 4450, 5071, 1910, 6953, 4872, 2116, 4865,\n",
            "                  6269, 3752, 1486, 3857, 1367,   54,  160, 2978, 7853, 3192, 4339,  556,\n",
            "                  2417, 3658,   68, 2331, 4688, 2154,  458, 3094, 6619, 2989, 1917, 3974,\n",
            "                  4847, 7235, 1072, 3032, 6172, 5717, 7186, 3665, 7913]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  306, 1767, 1500,  236,  654,  591,  572,  531, 1848, 1219,\n",
            "                       2, 1157,   25, 1128, 1910,  380,  653,  807]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  306, 1767, 1500,  236,  654,  591,  572,  531, 1848, 1219,\n",
            "                       2, 1157,   25, 1128, 1910,  380,  653,  807]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  306, 1767, 1500,  236,  654,  591,  572,  531, 1848, 1219,\n",
            "                       2, 1157,   25, 1128, 1910,  380,  653,  807]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 644,  254,  398, 1905, 1014,   52, 1037,  662,  985, 1299, 1923, 1265,\n",
            "                     755,  290,  673,  207,  436,  944,  796, 1221]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  306, 1500, 1767,  236,  654,  591,  531,  572,  380, 1848,\n",
            "                   390,  326,  807,   25, 1128, 1545, 1348,  964]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 790, 7398, 2536,  483,  989,  977, 1995, 6051, 6811, 8043,  135, 7214,\n",
            "                  6848, 7837,  821, 6480, 4009, 4392, 6062, 7483, 7807, 4679, 5484, 2551,\n",
            "                  6446, 4601, 6254, 1030,  707, 6717, 3206, 7045, 2250, 1634, 7060, 6631,\n",
            "                  4772, 2523,  610, 2267, 1202, 3753, 1200, 2009, 3256, 5062, 4676, 5680,\n",
            "                  4888, 1183, 2576, 3564,  742, 7227, 3348, 7382, 4268, 7069, 6526,  800,\n",
            "                  4422, 7854, 7027, 6703, 4738, 7152, 5382, 6088, 1231, 3596, 2412, 7238,\n",
            "                  4501, 2203, 3108, 2242, 5805,  867, 3555, 6833, 7002]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177, 1500,  236, 1767,  306,  654,  591,  531,  572, 1848,  380,\n",
            "                    1765,    2, 1192,  807,   25,  804, 1219, 1150]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177, 1500,  236, 1767,  306,  654,  591,  531,  572, 1848,  380,\n",
            "                    1765,    2, 1192,  807,   25,  804, 1219, 1150]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177, 1500,  236, 1767,  306,  654,  591,  531,  572, 1848,  380,\n",
            "                    1765,    2, 1192,  807,   25,  804, 1219, 1150]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 676,   68, 1320, 1493, 2027,  646,  687,   53,  447, 1288,  325,   88,\n",
            "                    1688,  340,  635,  593, 1255,  936, 1918,  521]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177, 1500,  236, 1767,  306,  654,  591,  531,  572, 1848,  380,\n",
            "                  1150, 1192,  807,  390, 1765,  326, 1348, 1219]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([5207, 6524, 2286, 4896, 5466, 1849, 2516, 3823, 2502, 3526, 6193, 6111,\n",
            "                  7951,  831, 4769,  526, 5363, 4913,  335, 2003,  603, 6953, 1859, 1974,\n",
            "                  1972, 1035, 5803, 6002,  558, 6444,   26, 2082,  499, 6554, 2775, 3988,\n",
            "                  4357,  538, 2854, 2785, 6472, 1583, 4928, 7942,  660, 4763, 6562, 3718,\n",
            "                  2212,  853, 4821, 3239, 6169, 2685, 2733, 4114, 7371, 3062,  250, 7536,\n",
            "                  6920, 5061, 5178, 1560,  441, 2111, 1943, 4734, 4109, 6875, 6202, 2122,\n",
            "                  2646, 3047, 7423, 1400, 7438,  245, 6663, 2876, 3666]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767, 1500,  654,  572,  591,  531,  306, 1848, 1192,\n",
            "                     380, 1765, 1150,    2,  807,  804, 1221, 1110]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767, 1500,  654,  572,  591,  531,  306, 1848, 1192,\n",
            "                     380, 1765, 1150,    2,  807,  804, 1221, 1110]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767, 1500,  654,  572,  591,  531,  306, 1848, 1192,\n",
            "                     380, 1765, 1150,    2,  807,  804, 1221, 1110]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1603, 1627,    6,  830, 1389, 1561, 1798,  447,  189, 1656,  772, 1668,\n",
            "                    1456,  311, 1612,  780, 1585, 1617,  200, 1471]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1767,  654, 1500,  591,  572,  531, 1848,  380, 1765,\n",
            "                  1192,  807, 1150,  306,  390,    2,  626,   29]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([2189, 2973, 2350,   49, 4518, 5439, 7194,   27, 7199,  473,  982, 6299,\n",
            "                  2017, 1683, 3094, 2823, 2783, 2417, 4869, 3398, 6358,  487, 2275, 2190,\n",
            "                  3701, 8023, 4960, 1293, 7250, 4383, 4199, 7315, 4637,  325, 8057, 6889,\n",
            "                   872, 4381, 2676, 7793, 4652, 7155,  786, 1090,  268, 5759,  243, 6675,\n",
            "                  3849, 3155, 3157,  171,  710, 1668, 3312, 5788, 2941, 6237, 6697, 5470,\n",
            "                  5373, 7542, 3027, 3731, 3163, 1482, 4901, 1971, 4487, 3135, 2999, 5562,\n",
            "                  6173, 7046,  933,  626, 7765, 4701, 1619, 2271, 5488]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572,  591,  531, 1848, 1500, 1192, 1765,\n",
            "                     380, 1150,    2,  807,  804, 1221, 1110,  467]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572,  591,  531, 1848, 1500, 1192, 1765,\n",
            "                     380, 1150,    2,  807,  804, 1221, 1110,  467]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572,  591,  531, 1848, 1500, 1192, 1765,\n",
            "                     380, 1150,    2,  807,  804, 1221, 1110,  467]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 162,  674,  638, 1447,  457,  168,  647,  178, 1766, 1813,  793,  375,\n",
            "                    1173,  527,  893,  828, 1573, 1104,  903, 1401]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572,  591, 1848,  531, 1765,  380, 1500,\n",
            "                  1150,  807, 1192,  467,  804, 2031,    2,   29]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([7100, 2961, 4494,  156, 1995, 2879, 4978,  265, 6932, 7030, 3739, 6436,\n",
            "                  4797, 1272, 4222,  348, 1602, 1894, 1502, 4381, 4353, 3440, 2338, 1322,\n",
            "                  1701,   20, 1165, 3805, 7379, 5578, 2871, 1198, 5324, 6957, 5398, 3972,\n",
            "                  7259, 3103, 2566, 3124, 5799, 3893, 3023, 4701, 3409, 3091,  328, 7113,\n",
            "                  3002, 7768, 8101, 5868, 7725, 4785, 2183, 2570, 2703, 7524,  683, 6590,\n",
            "                  4710, 3630, 6979, 6009, 4029, 5174,  786, 2590, 1481, 2835, 3713, 4606,\n",
            "                  1216, 6784, 3564, 2706, 5176, 3321, 2088, 2851, 6692]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765, 1192,  380,\n",
            "                    1150,  807, 1221,  804,    2,  467, 1110, 1412]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765, 1192,  380,\n",
            "                    1150,  807, 1221,  804,    2,  467, 1110, 1412]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765, 1192,  380,\n",
            "                    1150,  807, 1221,  804,    2,  467, 1110, 1412]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1441, 1334,  860,  748, 1654,  482,  917,  277,  609, 1300,  631,  526,\n",
            "                    1977, 1540, 1935, 1899,  575, 1830,  665, 1294]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1767,  654, 1848,  572,  591,  531, 1765,  380,  467,\n",
            "                   807,  804, 1192, 1150, 1412, 1721,  621,  626]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([2509, 7829, 4066, 6000, 1236, 4470, 1944, 4306, 1051,  591, 3067, 3832,\n",
            "                  7094, 7788, 3351, 3141, 4677, 2488, 7064, 7846, 3987, 4425, 3133, 6943,\n",
            "                  7549, 1794, 1228, 3325, 7938, 6104, 7155,   95, 1578, 2943, 3589, 7676,\n",
            "                   124, 6141, 1179, 7309, 4568, 7069, 7354, 4530, 6402, 4344, 4353, 7902,\n",
            "                  1902, 4222, 6517, 3799, 1075, 1435, 4720, 5146, 7123, 5258, 4249, 2890,\n",
            "                  2010, 4435, 3402, 1123, 2533, 7235, 3931, 2464,  820, 5543, 5238, 7386,\n",
            "                  5475,  455, 2482, 5209, 1674, 8073, 6770, 3015, 7562]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804, 1221,  807,  467,    2, 1110, 1150,  626]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804, 1221,  807,  467,    2, 1110, 1150,  626]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804, 1221,  807,  467,    2, 1110, 1150,  626]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([2022, 1673, 1924, 1441, 1193,  605,   63, 1684, 1413,  251, 1659, 1325,\n",
            "                    1640,  677, 1145,   24,  131, 1473,  413, 1299]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1767,  654, 1848,  572,  591,  531, 1765,  380,  467,\n",
            "                   804, 1192,  621,  807,    2, 1110, 1221,  626]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([2183, 4819,  323, 3144, 6058, 3429,  314, 4892,  201, 5737,  981, 5317,\n",
            "                  4912, 1150, 4573, 5982, 2507, 5946, 7507, 7626, 2825, 1289, 7156, 5697,\n",
            "                  5079, 7696, 2777, 6310, 2818, 4866, 1058, 7033, 7288, 8164,  103, 4272,\n",
            "                  4754, 5212,  143, 7982, 3035, 7993,  694, 4790,  807,  237, 5173, 5391,\n",
            "                  6405, 7015, 7415, 6361,  889, 3123, 4570, 6454, 7935, 6779, 6931, 4625,\n",
            "                  6562, 1200, 1054, 7972, 6010, 6241, 3625, 5272, 3826, 7419, 3548, 6061,\n",
            "                  7462, 5018, 1762, 2597, 4224, 6624, 1351, 3929, 7646]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804, 1221,  467,  807,    2,  621, 1110, 1927]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804, 1221,  467,  807,    2,  621, 1110, 1927]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804, 1221,  467,  807,    2,  621, 1110, 1927]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 262,  895, 1162,  266, 1492, 1629,  419,  878,  340,  863,  778, 1355,\n",
            "                    1385, 1731, 1790,  115, 1602, 1459, 1373, 1585]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380,  467,\n",
            "                   804,  621, 1192, 1221,    2,  626, 1927,  807]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1630,  780, 3248, 4499, 2817, 2608, 5314, 2676, 2585, 3903,  102, 4744,\n",
            "                  5336,  155, 8127, 6770, 6231,  639, 4398, 2402, 6075, 5008, 8096, 2493,\n",
            "                  1424, 6117, 2117, 6638, 2054, 2846,  828, 8161, 3457, 6272, 6294, 2183,\n",
            "                  5132, 7432, 6420, 5071, 1827, 4843, 4913, 5466, 1932,  782, 3616,  131,\n",
            "                  4321, 1523, 5698, 6276, 2482, 3681, 1804, 2001,  986, 6510, 3867, 3385,\n",
            "                  7870, 5934, 3394, 3492, 6230, 4076, 5573, 6972, 2456, 1745, 8070, 7803,\n",
            "                  5337,  484, 4067, 2718, 3167, 6747, 2257, 4442, 6702]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (8): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467, 1221,    2,  621,  807, 1927, 1110]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467, 1221,    2,  621,  807, 1927, 1110]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467, 1221,    2,  621,  807, 1927, 1110]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1422,  226,  211, 1836,  445, 1817,  825,  852, 1809, 1478, 1577, 1509,\n",
            "                    1248,  683, 1801,  191,  339, 1802,  298, 1396]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380,  467,\n",
            "                   804,  621, 1192,    2, 1221, 1927,  626, 1110]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1207, 1191, 3928, 2324, 8016, 2501, 3343,  273, 6292, 4546, 2283, 7004,\n",
            "                  1953, 4860, 3946, 3851, 5604, 6950, 5887, 8170, 1630, 5391,  919, 6658,\n",
            "                   333, 6761, 4563,  459,  357, 3909, 1488, 7734, 3916,  318, 6377,  856,\n",
            "                  1593, 4360, 3024, 4040, 2928, 7088, 3627, 6283, 4118, 3990, 7210,  554,\n",
            "                    83, 1680, 2165, 5790, 5193, 1971, 1802, 5900, 4928,  627, 2943,    2,\n",
            "                  2040, 3396, 2720, 4486, 7831, 7174, 7874, 6858,  378, 1061, 6846,  458,\n",
            "                  6734,  680, 5310, 4394,   19, 5620, 5049, 3459, 7082]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (9): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  572,  654, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467, 1221,  621,    2, 1927,  626,  807]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  572,  654, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467, 1221,  621,    2, 1927,  626,  807]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1767,  572,  654, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467, 1221,  621,    2, 1927,  626,  807]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1973, 1357, 1977, 1974, 1196, 1944, 1952,  170, 1941, 1920, 1922, 1971,\n",
            "                    1218,  942,  643, 1963, 1928, 1942, 1014,  843]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1767,  654,  572, 1848,  591,  531, 1765,  380,  467,\n",
            "                   621,  804,    2, 1192, 1221,  626, 1927, 1915]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([5990, 6235, 8044, 6403, 2924, 3441, 6640,  161,  575, 7547, 3469, 5217,\n",
            "                  5559, 4260, 6153, 2492, 6207,  518, 3407, 5052, 1810, 3480, 3166, 5722,\n",
            "                  1464, 4528, 7175, 5397, 7619, 4932, 4326, 3542, 6683, 8141, 1216, 1681,\n",
            "                  5607, 5858, 1869,  438, 3897, 6528, 2452, 1486, 1399, 7584, 7881, 5826,\n",
            "                  3302, 3991, 1123, 3891, 3533, 6427, 1499, 4772, 4507, 3715, 8122, 8009,\n",
            "                  2178, 3338, 4156, 5309, 7726, 4509, 6729, 8040, 2347, 5851, 1972,  940,\n",
            "                  7226, 6214, 2928, 1446, 6645,  410, 3290, 1773, 4469]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (10): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  572,  654, 1767, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467,  621, 1221,    2, 1927,  626,  266]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  572,  654, 1767, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467,  621, 1221,    2, 1927,  626,  266]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  572,  654, 1767, 1848,  591,  531, 1765,  380, 1192,\n",
            "                     804,  467,  621, 1221,    2, 1927,  626,  266]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1407,  757, 2039, 1468,  142, 1350,  494, 1949, 1649, 1471,  564, 1469,\n",
            "                    1391,  272, 1338, 1424, 1755, 1707, 2023, 1402]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  467,\n",
            "                   804,  621,    2, 1192, 1221,  626, 1915,  266]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 385, 6196, 1159, 2695, 1117, 5041, 4515, 1727, 3999, 6498, 8144, 4865,\n",
            "                  6958,   36, 4262, 3668, 2858, 3819, 4392, 6432, 6199, 4922,  947, 4397,\n",
            "                  3694, 3769, 2684, 5355, 3336, 5398, 7330, 7661, 6897,   79, 8040, 1106,\n",
            "                  5946, 4425, 4873, 4969,  958, 3549, 3080, 5930, 4117, 5506, 2283, 4496,\n",
            "                   545, 2228, 5659, 8069, 3103, 1627, 3570, 5878, 6174, 3655, 6867, 4707,\n",
            "                    10, 5705, 2937, 1601, 6564, 4274, 8148, 7056, 1928, 2165, 2950, 3021,\n",
            "                  7329, 4195, 2471,  683, 1848, 2935, 3920, 7343, 5486]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (11): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  804,\n",
            "                     467, 1192,  621, 1221,    2,  626,  266, 1927]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  804,\n",
            "                     467, 1192,  621, 1221,    2,  626,  266, 1927]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  804,\n",
            "                     467, 1192,  621, 1221,    2,  626,  266, 1927]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1023, 1945, 1866, 1746,  545, 1705, 1740,  548, 1063, 1522, 1734,  694,\n",
            "                    1251, 1281, 1549, 1947, 1930, 1527,  670,  463]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  467,\n",
            "                   804,  621,    2,  626, 1221, 1915,  266, 1192]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([4150, 5983, 6226, 8167, 3011, 1142, 4392, 3643,  312,  396, 1252, 4506,\n",
            "                  6614, 6521, 5732, 3454, 2818, 6487, 6882, 6239, 1394, 6927, 6532, 7362,\n",
            "                  3364,  544, 6509, 5730,  114, 7554, 6450, 2829, 4311, 6413, 4774, 4871,\n",
            "                  4093, 2594, 2611,  299, 3412, 7095, 6960, 4916, 2909, 6946, 3033,   64,\n",
            "                  2043,  664, 5970, 7448, 3874, 4856, 5671, 6616, 3724, 3899,  374, 4718,\n",
            "                  4381, 6173, 1647, 6988, 3249, 1891, 4664, 2911, 1097, 5891, 3029, 5181,\n",
            "                  6211,  890, 1567, 3797, 6390, 4660, 8012, 5136, 5731]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (12): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  804,\n",
            "                     467, 1192,  621, 1221,  626,    2,  266, 1927]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  804,\n",
            "                     467, 1192,  621, 1221,  626,    2,  266, 1927]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  804,\n",
            "                     467, 1192,  621, 1221,  626,    2,  266, 1927]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1780, 1675,  150, 1284, 1713, 1202,  570, 1102,   98, 1701,  659, 1312,\n",
            "                     795, 1691,  465, 1707, 1715,  746, 1666, 1543]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  380,  804,\n",
            "                   467,  621,    2,  626,  266, 1221, 1915,  559]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([3881, 5256, 5583, 7639, 2342, 1953, 3708, 1549, 6611, 2706, 3264, 1338,\n",
            "                  4381, 4906,  134, 1521, 7691, 1511, 2605, 6641,  946, 3589, 2710, 2623,\n",
            "                   983, 5444, 5647, 7133,  790, 7805, 2504, 4654, 1767, 2476, 5762, 3725,\n",
            "                  2516, 4544, 4847, 3976, 3742, 4938, 2828, 1190, 2140, 5101, 6631, 4442,\n",
            "                  5702, 7373, 2341, 5741,  207, 7424, 5796, 1124, 6762, 1833, 7178, 5184,\n",
            "                  1960, 2427, 6489, 4605, 7241, 3637,  972, 7032, 7552, 3073, 1726, 5882,\n",
            "                  2991, 6608,  168, 3440, 4509, 8172, 2000, 3938, 1406]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (13): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767,  591, 1848, 1765,  531,  380,  804,\n",
            "                     467,  621, 1192,  626, 1221,  266,    2,  371]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767,  591, 1848, 1765,  531,  380,  804,\n",
            "                     467,  621, 1192,  626, 1221,  266,    2,  371]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767,  591, 1848, 1765,  531,  380,  804,\n",
            "                     467,  621, 1192,  626, 1221,  266,    2,  371]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([2020,   62,  281,  131, 1459, 1094,  798, 1279, 1615,  234, 1381,  307,\n",
            "                     487,  408, 1954,  306,  294, 1992, 1245, 1286]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767, 1848,  591, 1765,  531,  467,  804,\n",
            "                   380,  621,  626,    2,  266,  559, 1915, 1221]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([6059, 1025, 7204, 2057, 1820, 7970, 1622,  378, 1652, 6104, 2444, 4269,\n",
            "                  3064, 4580, 6950, 6675, 4277, 3961, 5584, 3907, 3717, 6339, 3141, 6008,\n",
            "                   316, 4608, 2148, 3127, 4086, 7497, 8076, 7557, 6520, 5825, 3354, 3682,\n",
            "                  2317, 3743, 4012, 2538, 1854, 4408, 7822, 5012, 2722, 6030, 2349, 5664,\n",
            "                  2417, 1696, 7232, 7994, 4749, 3183, 3137, 6984, 2506, 7965, 6542, 7327,\n",
            "                  4149, 5729,  623, 5245,  911, 2169, 3670, 1181, 6847, 8093, 4191, 5678,\n",
            "                  3260, 7081, 1289, 4757, 3843, 1505, 5893,   67, 7185]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (14): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767,  591, 1848, 1765,  531,  804,  467,\n",
            "                     380,  621,  626, 1192,  266,    2, 1221,  371]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767,  591, 1848, 1765,  531,  804,  467,\n",
            "                     380,  621,  626, 1192,  266,    2, 1221,  371]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572, 1767,  591, 1848, 1765,  531,  804,  467,\n",
            "                     380,  621,  626, 1192,  266,    2, 1221,  371]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1388, 1147, 2044,  282,   34, 1312, 1108,  962, 1746,  489, 1624, 1295,\n",
            "                     709, 1044,  125,  369, 1473, 1391, 1348, 1073]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1767, 1848, 1765,  531,  467,  804,\n",
            "                   380,  621,  626,    2,  559,  266, 1221, 1773]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([3223, 1191,  628, 8116, 3336, 3715,  202, 4716, 7562, 2834, 5753, 2111,\n",
            "                  6362, 3772, 3791, 1081, 7458, 4945, 5930, 7745, 7621, 1625, 2574, 5926,\n",
            "                  5132, 3757, 6401, 2917, 5277, 3786, 2255, 7906, 6762, 5266, 6704, 8118,\n",
            "                  2974, 5731, 5937, 1383, 4269, 6302,  736, 5885, 4944, 5940, 6205, 4254,\n",
            "                  4032,  586, 3975, 3462, 1601, 1019, 5084, 4736, 3467, 5354, 1970, 4074,\n",
            "                  8101, 5671, 2245, 2706, 3378, 5693, 5540, 3495, 1895, 4388, 3852, 2738,\n",
            "                  2000, 6336, 2835, 3799, 7063, 4402, 2109,  666, 6709]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (15): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1767, 1848, 1765,  531,  804,  467,\n",
            "                     380,  621,  626,  266,  845,  559,    2,  371]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1767, 1848, 1765,  531,  804,  467,\n",
            "                     380,  621,  626,  266,  845,  559,    2,  371]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1767, 1848, 1765,  531,  804,  467,\n",
            "                     380,  621,  626,  266,  845,  559,    2,  371]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([  54,  551, 1422, 1218, 1976,  180,  361,  656, 1120, 1017,  326,  745,\n",
            "                    1255, 1633, 1321, 1736, 1996,  425, 1683,  332]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1767, 1765,  531,  467,  804,\n",
            "                   621,  380,  626,  559,    2,  266, 1773,  845]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([3014, 6934, 4254, 2297, 1350, 5412, 7943, 6728, 7507, 5850, 2693, 7982,\n",
            "                  7662, 2587, 1228, 2762, 6668, 4211, 1736, 7730,  417, 6653, 1245, 6286,\n",
            "                  5794, 1090, 6711, 4406, 4344, 3363, 5300,  650, 6416, 7608, 6696, 6361,\n",
            "                     7, 3145, 4973, 7438, 2114, 1958, 2619, 3093, 7424, 7501, 4382,  188,\n",
            "                   456, 6734, 6047, 1682,  720, 1690, 3633,   23,  765, 6947, 3444, 6232,\n",
            "                  1078, 6727, 3686, 7945, 6095, 6164, 3857, 7241, 1115, 3981, 5917, 2703,\n",
            "                  2404, 6018, 5233,  780, 5658, 5891, 7062, 6081, 1098]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (16): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1767, 1765,  531,  804,  467,\n",
            "                     621,  380,  626,  845,  266,  559,  371,    2]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1767, 1765,  531,  804,  467,\n",
            "                     621,  380,  626,  845,  266,  559,  371,    2]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1767, 1765,  531,  804,  467,\n",
            "                     621,  380,  626,  845,  266,  559,  371,    2]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1049, 1227, 1611,  541, 1428, 1123, 1597, 1012, 1044, 1467, 1077, 1431,\n",
            "                     374,  808,    6,  424,  159,   94, 1030,  998]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1765, 1767,  467,  531,  804,\n",
            "                   621,  380,  626, 1773,  559,  845,  266,    2]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([5062, 6712, 6691, 3139, 7475, 6564, 3250, 2409, 6644, 4422, 3372, 1782,\n",
            "                  1454, 1993, 7314, 7844, 2982, 5501, 1368,  732,   24, 1229, 6237, 5428,\n",
            "                  3041, 4500, 4937, 6481, 1853,  539,  629, 4055, 3208,  471, 6437, 6523,\n",
            "                  5552, 3389, 2219, 7220, 5416, 6537, 7003, 6483, 2361, 5052, 7133, 7922,\n",
            "                  7230, 5245,  706, 4195, 3917, 2439, 4228, 5933,  266, 7159, 5623, 2253,\n",
            "                  7139, 2303, 1913, 4428, 6366, 6561,  912,  797, 2977,  481, 4182, 7784,\n",
            "                  1707, 4869, 1204,  435, 7863, 7252, 3636,  718, 1961]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (17): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1765, 1767,  804,  467,  531,\n",
            "                     621,  380,  845,  626, 1773,  266,  276,  371]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1765, 1767,  804,  467,  531,\n",
            "                     621,  380,  845,  626, 1773,  266,  276,  371]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1765, 1767,  804,  467,  531,\n",
            "                     621,  380,  845,  626, 1773,  266,  276,  371]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 365, 1319, 1429, 1389,  503, 1974,  409, 1624,  787, 1044,  884, 1380,\n",
            "                     823, 1315, 1890,   23, 1892, 1344, 1899, 2042]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1765, 1848,  467,  804,  531, 1767,\n",
            "                   621, 1773,  276,  626,  380,  845,  559,  266]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([7868, 3480, 1210, 3927, 5805, 7919, 5431, 5851, 1961, 4159, 2160, 1475,\n",
            "                  3946, 7565, 6748, 4973,  532, 4191, 3795, 1042, 1431, 1891,  565, 7972,\n",
            "                  4420, 1209, 1040,  378, 7662, 7593, 7272, 1308,   68,  253, 1912, 2745,\n",
            "                  5073, 1974,  612, 2687, 7424, 4247, 2562, 4388, 6207,  650, 2356, 4914,\n",
            "                  7964, 6771, 4169, 6433, 5465, 4011, 3912,  320, 1963, 5097, 2189,  270,\n",
            "                  3813, 5314, 7900, 2421, 6563, 7779, 6875, 8053, 5760, 4404, 4701, 6717,\n",
            "                  7771, 5362, 4214, 5725, 1699, 4946, 6129, 5607, 2038]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (18): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1765,  804, 1767,  467,  531,\n",
            "                     621,  276,  845, 1773,  380,  626,  266,  559]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1765,  804, 1767,  467,  531,\n",
            "                     621,  276,  845, 1773,  380,  626,  266,  559]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1848, 1765,  804, 1767,  467,  531,\n",
            "                     621,  276,  845, 1773,  380,  626,  266,  559]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1173, 1326,  190,  809, 1104,  771, 1595, 1926, 1797,  615,  528, 1898,\n",
            "                    1569,  254,  196, 2044,  117, 1580, 1400, 2006]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1765, 1848,  467,  531,  804,  276,\n",
            "                   621, 1773, 1767,  845,  626,  559,  380,  266]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([7950, 6371, 1405,  349, 2888, 2740, 5185, 7111, 1082, 6914, 4161, 1456,\n",
            "                  2321, 1184, 4436, 5239, 2540,  520, 3047, 3405, 6346, 4730, 7758, 3849,\n",
            "                  3810, 2380,    4, 1391, 5770, 2938, 3852, 5619, 3592, 2386, 3487,  415,\n",
            "                  7127, 3346, 4304, 5949, 3098, 4965, 5366, 7008, 5912, 7345, 3053, 3557,\n",
            "                  5051, 1323, 4752,  102,  390,  725, 6304, 7478, 5071, 4155, 4079, 2010,\n",
            "                  4662, 6860, 8070, 2391,  825, 3163, 5238, 7178, 4613, 4624,  773, 6331,\n",
            "                  2900, 2360, 5195, 6308, 4334, 1439, 2709, 1093, 5305]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (19): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1765, 1848,  276,  467,  804,  531,\n",
            "                     845, 1767,  621, 1773,  380,  626,  559,  371]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1765, 1848,  276,  467,  804,  531,\n",
            "                     845, 1767,  621, 1773,  380,  626,  559,  371]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1765, 1848,  276,  467,  804,  531,\n",
            "                     845, 1767,  621, 1773,  380,  626,  559,  371]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 154, 1414, 1068, 1485, 1936,  144, 1878, 1484, 1669,  582,  599, 1705,\n",
            "                    1480,  508, 1502,  150, 1908,  184, 1490, 1517]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654,  572,  591, 1765,  276, 1848,  531,  467,  804,\n",
            "                  1773,  621,  845, 1346, 1767,  559,  626,  266]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([6983, 3251, 4333, 7043, 2272, 1990, 5034, 4794, 2747, 5740, 1289,  595,\n",
            "                  2201, 1184, 2800, 2674, 7106, 4045, 6615, 2401, 7342, 1795, 4296, 6707,\n",
            "                  4231, 7056, 2831, 4713, 7853, 4116, 2215, 5124, 1823, 4706, 4839,   23,\n",
            "                  1987,  968, 3775, 7023, 1528, 4924, 4403, 7498, 5200, 5560, 6193, 5247,\n",
            "                  5089,  895,    6, 5188, 1009, 6242,  210,  721, 4874, 3049,  980, 2289,\n",
            "                  5466, 6338,  536,  708,   71,  878, 5171, 4553,  891, 5405, 4326, 5041,\n",
            "                  1667, 1767,  539, 7879, 5634, 2560,   90, 1230, 6041]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (20): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  591,  572, 1346, 1765,  276,  734, 1848,  614,\n",
            "                     804, 1279,  467, 1583,  845, 1773,  531,  621]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  591,  572, 1346, 1765,  276,  734, 1848,  614,\n",
            "                     804, 1279,  467, 1583,  845, 1773,  531,  621]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654,  591,  572, 1346, 1765,  276,  734, 1848,  614,\n",
            "                     804, 1279,  467, 1583,  845, 1773,  531,  621]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 930, 1902,  931, 1903, 1047, 1942,  981, 1961,  331,  709, 1021, 1969,\n",
            "                      81, 1286, 1938, 1742, 1922,  534,  956, 1022]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236,  654, 1346,  572,  591,  276, 1765,  734,  614, 1279,\n",
            "                   531, 1848,  467,  804, 1952, 1583, 1773,  262]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([4792, 2100, 5233, 1282, 2432, 4934, 6582, 7803, 1369, 4094, 6059, 4283,\n",
            "                  3142, 5383, 6898, 1378, 5093, 2978, 8077, 3729, 5619, 4996, 2770, 2197,\n",
            "                  7514, 6804, 2735, 3298, 1238, 8190, 7136, 4084, 1423, 4927, 6131, 8131,\n",
            "                  2710, 7211, 2500, 2793, 2389,  548, 8098, 5311, 7352, 5837, 1340,  982,\n",
            "                  3293, 3175, 6530, 4533, 4148, 3194, 2166, 1222, 5097, 5861, 1430, 1932,\n",
            "                  7951, 7697, 7876, 5448, 5157, 7472, 2799, 1679, 1550, 3279, 3181, 5915,\n",
            "                  2740,  227, 7416, 3999, 1634, 6304,  629, 2199, 5632]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (21): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654, 1346,  734,  591,  572,  614, 1279,  276, 1583,\n",
            "                     262, 1952, 1765,  396, 1436, 1573, 1848,    7]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654, 1346,  734,  591,  572,  614, 1279,  276, 1583,\n",
            "                     262, 1952, 1765,  396, 1436, 1573, 1848,    7]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236,  654, 1346,  734,  591,  572,  614, 1279,  276, 1583,\n",
            "                     262, 1952, 1765,  396, 1436, 1573, 1848,    7]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 817,  102,  933, 1630, 2043,  953,  437, 1016,  483,  802,  952, 1433,\n",
            "                     675,  456,  654, 1921,  509,  457,  495,  459]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177,  236, 1346,  654,  734,  614, 1279, 1583,  262,  276, 1952,\n",
            "                   396,  591, 1765,  572, 1573,    7, 1436, 1326]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([6399, 1724, 4364, 7519, 5577, 7946, 5617, 6052, 7314, 2953,  299, 1973,\n",
            "                  7332, 2116, 4244, 2538, 4640, 4614,  973,  847, 5691, 5447, 6806, 6425,\n",
            "                  5863, 3825, 1031, 4035, 3480, 5853, 7869, 5913, 1696, 7681, 5995, 6761,\n",
            "                  3260, 5155, 2859, 2637, 1609,  146, 7501, 3098, 3368, 5130, 2383, 3901,\n",
            "                  6249, 7749, 3920,  751, 3660, 2164, 1133, 5859, 5077, 4945, 4436,  637,\n",
            "                  6709, 2902, 1554, 4348, 4210, 5131,  982, 2700, 1730, 5317, 6328, 3083,\n",
            "                  5275, 7283, 1699, 4219, 5650, 5068, 3767, 3031,  681]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (22): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1346,  734,  654, 1279, 1583,  614,  262, 1952,  396,\n",
            "                     276, 1436, 1326,  591,    7, 1573,  745, 1765]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1346,  734,  654, 1279, 1583,  614,  262, 1952,  396,\n",
            "                     276, 1436, 1326,  591,    7, 1573,  745, 1765]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1359, 1177,  236, 1346,  734,  654, 1279, 1583,  614,  262, 1952,  396,\n",
            "                     276, 1436, 1326,  591,    7, 1573,  745, 1765]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 484, 1540, 1113, 1733, 1116, 1283,  239, 1597,  592, 1559,  129, 1790,\n",
            "                     509,  194,  254,  840, 1558,  209, 1737,  881]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1359, 1177, 1346,  236,  734, 1583,  262, 1279,  614,  654, 1952,  396,\n",
            "                  1326,    7, 1436,  276, 1573,  745, 1869,  125]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1904, 1750, 3040, 6533, 6032, 6944, 1399, 5403, 6491, 3052, 2344, 5201,\n",
            "                  6941, 4715, 7592, 1536,  483, 3600, 4219, 2812, 6636, 3336, 7479, 2740,\n",
            "                  4156,  580, 7067,  878, 3192, 3665, 1306, 4267, 4621, 3551, 5898,  931,\n",
            "                   138, 4584, 7680, 7846, 6950, 7419, 2416, 6242, 2091, 6874, 6399, 7550,\n",
            "                  6190, 5557, 4531, 5238, 4831,  506, 6441, 8080, 5235, 3238, 4224, 3836,\n",
            "                  2698, 4255, 2006, 4049, 4162, 5909, 8146, 3389, 3956, 5005, 3529, 1243,\n",
            "                  2426, 7147,  199, 7613, 1052, 2520, 5169, 8070,  308]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (23): OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1346, 1359, 1177, 1583,  734,  236,  262, 1279, 1952,  614,  396, 1326,\n",
            "                    1436, 1573,    7,  263, 1869,  745,  276,  654]))\n",
            "            (v_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1346, 1359, 1177, 1583,  734,  236,  262, 1279, 1952,  614,  396, 1326,\n",
            "                    1436, 1573,    7,  263, 1869,  745,  276,  654]))\n",
            "            (q_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor, salient_indices=tensor([1346, 1359, 1177, 1583,  734,  236,  262, 1279, 1952,  614,  396, 1326,\n",
            "                    1436, 1573,    7,  263, 1869,  745,  276,  654]))\n",
            "            (out_proj): W4A4Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([ 364, 1487,  665,  868,  656,  349,  701,  673,  381,  653,  690,  696,\n",
            "                     691,  864,  672,  340,  649,  666,  327,  353]))\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): W4A4Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([1346, 1359,  734, 1583,  262, 1279,  614, 1952,  396, 1326, 1436,    7,\n",
            "                  1573,  263, 1219,  125,  745, 1776,  892, 1869]))\n",
            "          (fc2): W4A4Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None, salient_indices=tensor([4752, 8189, 5672, 3500, 7594, 3971,  542,  150, 1447, 7560,  219, 7182,\n",
            "                  2114,   32, 6416, 2298, 3147, 3136,  965,  100, 3157, 5362, 5867, 1654,\n",
            "                  3359, 5303, 5217, 4022, 2029, 2412, 6027, 1349, 2221, 3776, 6137, 2192,\n",
            "                  6729, 7736,   62, 4706, 5568, 2705,  993, 7300, 8027, 7380, 3596, 2314,\n",
            "                  5584,  653, 1185,  580, 5398, 5656, 7042, 6166, 1261, 7174, 6543, 4165,\n",
            "                   986,  858, 8187, 1125, 1610,  108, 4989, 3234, 5799,  503, 4240, 6017,\n",
            "                  6793, 7700, 1920, 3372, 2675,  799,  160, 1953, 4805]))\n",
            "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p@torch.no_grad()\n",
        "def pseudo_quantize_model_salient_weight_fp16(\n",
        "    model, w_bit, q_group_size, input_feat\n",
        "):\n",
        "    for n, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            importance = sum(input_feat[n]).float()\n",
        "\n",
        "            ############### YOUR CODE STARTS HERE ###############\n",
        "\n",
        "            # Step 1: Find 1% of the salient weight channels according to importance (hint: use torch.topk())\n",
        "            outlier_indices = torch.topk(importance, m.weight.data.shape[1]//100)[1]\n",
        "            assert outlier_indices.dim() == 1\n",
        "\n",
        "            ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "            # Back up the values of the salient weight channels\n",
        "            outlier = m.weight.data[:, outlier_indices].clone()\n",
        "\n",
        "            m.weight.data = #pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n",
        "\n",
        "            ############### YOUR CODE STARTS HERE ###############\n",
        "\n",
        "            # Step 2: Restore the 1% salient weight channels to their original FP16 values\n",
        "            m.weight.data[:, outlier_indices] = outlier\n",
        "\n",
        "            ############### YOUR CODE ENDS HERE #################"
      ],
      "metadata": {
        "id": "uYp8CBdUtgG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViOypjmmiEYn"
      },
      "source": [
        "## FP16 Model Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5V66zhAiEYn"
      },
      "source": [
        "Let's first check the performance of the original FP16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4QVy1PUiEYn",
        "outputId": "db80a843-7a55-4ab3-d7db-d558794a33df",
        "colab": {
          "referenced_widgets": [
            "27d329da8d934f258323e1977002cebd"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27d329da8d934f258323e1977002cebd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_fp16 = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz9SOnENiEYn",
        "outputId": "4ac9140d-a005-4d36-deb4-06dbe54c519c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating...: 100%|██████████| 40/40 [00:09<00:00,  4.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model (fp16) perplexity: 5.822948932647705\n"
          ]
        }
      ],
      "source": [
        "ppl_fp16 = evaluate(model_fp16)\n",
        "print(f\"Original model (fp16) perplexity: {ppl_fp16}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxmMRgUTiEYn"
      },
      "source": [
        "We then quantize the model to W8A8 and check the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noaI-0_HiEYn"
      },
      "source": [
        "## Naive W8A8 Quantized Model Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl0ILrWHiEYn",
        "outputId": "13f46cac-ce9c-47e7-be01-844c03bf7d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model_w8a8 = quantize_llama_like(model)\n",
        "print(model_w8a8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKhz6Bm4iEYn",
        "outputId": "bbd51ed2-bce1-4dc0-f1ef-573b19d57d73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating...: 100%|██████████| 40/40 [00:07<00:00,  5.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive W8A8 quantized model perplexity: 5.931240558624268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ppl_w8a8 = evaluator.evaluate(model_w8a8)\n",
        "print(f\"Naive W8A8 quantized model perplexity: {ppl_w8a8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IbHJoeMiEYo"
      },
      "source": [
        "We can see there is a perplexity increase. We then use SmoothQuant to quantize the model and check the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6dXTdUSiEYo"
      },
      "source": [
        "## SmoothQuant W8A8 Quantized Model Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjjyubz9iEYo",
        "outputId": "37ecd04a-bd2c-4b6b-eea1-cd21c9e403b8",
        "colab": {
          "referenced_widgets": [
            "321b7c77ef984e6f82c74c8b6b16e558"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "321b7c77ef984e6f82c74c8b6b16e558",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (k_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (v_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (up_proj): W8A8Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (down_proj): W8A8Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
        "smooth_lm(model, act_scales, 0.85)\n",
        "model_smoothquant_w8a8 = quantize_llama_like(model)\n",
        "print(model_smoothquant_w8a8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdO2Y7U6iEYo"
      },
      "source": [
        "We can see the smoothed model has a lower perplexity which is close to the FP16 model's. This is because SmoothQuant smooths the outliers in activations and balances the quantization difficulty of activations and weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCcS-GAqiEYo",
        "outputId": "028728b7-afa4-4928-86aa-c054556d82b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating...: 100%|██████████| 40/40 [00:08<00:00,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SmoothQuant W8A8 quantized model perplexity: 5.85634183883667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ppl_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
        "print(f\"SmoothQuant W8A8 quantized model perplexity: {ppl_smoothquant_w8a8}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc13bd42fa7148ddb4151cb79fde159d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2bfa6e8f0ef4bc0bebdbd4e07c6ffda",
              "IPY_MODEL_68cb7f43a3fb4bc78da3f1eb10fa71f4",
              "IPY_MODEL_05889ac04de0418e8da134931c68086c"
            ],
            "layout": "IPY_MODEL_c78ffba17b84469ba3671d2690287bfd"
          }
        },
        "d2bfa6e8f0ef4bc0bebdbd4e07c6ffda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_273cb8d24d3b4394a77baff8efc718e1",
            "placeholder": "​",
            "style": "IPY_MODEL_9b837dc2e80f414581f7f7255b5ba9fe",
            "value": "config.json: 100%"
          }
        },
        "68cb7f43a3fb4bc78da3f1eb10fa71f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cd145bf78cc43059cc0d49e92237b18",
            "max": 653,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb664b00e46b4c159dd6a3d87ef686a4",
            "value": 653
          }
        },
        "05889ac04de0418e8da134931c68086c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da40483996524c0da676d5902af5962e",
            "placeholder": "​",
            "style": "IPY_MODEL_3267cc14e67e40119f3c5e526c432b63",
            "value": " 653/653 [00:00&lt;00:00, 37.6kB/s]"
          }
        },
        "c78ffba17b84469ba3671d2690287bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273cb8d24d3b4394a77baff8efc718e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b837dc2e80f414581f7f7255b5ba9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cd145bf78cc43059cc0d49e92237b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb664b00e46b4c159dd6a3d87ef686a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da40483996524c0da676d5902af5962e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3267cc14e67e40119f3c5e526c432b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cab584a76d874ce7949b6e9eb44dbbaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3962b1a744934923a60878dc1b45ec60",
              "IPY_MODEL_e0f1e0bcc2bd410aaabeac56a596101f",
              "IPY_MODEL_419c6f11ddc14c0f98691afb93fdc1b5"
            ],
            "layout": "IPY_MODEL_7801d8220f5147aa99f50d4933934c40"
          }
        },
        "3962b1a744934923a60878dc1b45ec60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec66d48a16134e5785a0c2cc1490c560",
            "placeholder": "​",
            "style": "IPY_MODEL_4b88a0f0d4bc40a3b1886d001e32a8f1",
            "value": "config.json: 100%"
          }
        },
        "e0f1e0bcc2bd410aaabeac56a596101f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38405eaa02984c2687a43011c922924f",
            "max": 653,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37cddfde58b84add9f6a2fafe399eb5d",
            "value": 653
          }
        },
        "419c6f11ddc14c0f98691afb93fdc1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_886f1b48137e44a6aaabdf687e00cd17",
            "placeholder": "​",
            "style": "IPY_MODEL_216a8226ddc340e38a4d44aca034a172",
            "value": " 653/653 [00:00&lt;00:00, 28.9kB/s]"
          }
        },
        "7801d8220f5147aa99f50d4933934c40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec66d48a16134e5785a0c2cc1490c560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b88a0f0d4bc40a3b1886d001e32a8f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38405eaa02984c2687a43011c922924f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37cddfde58b84add9f6a2fafe399eb5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "886f1b48137e44a6aaabdf687e00cd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216a8226ddc340e38a4d44aca034a172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a68d482395ea47c0a1245432aa2ddb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_963d34f7439f42a6be5449a3472744d1",
              "IPY_MODEL_e88335905554416b93dd90ac1e0f93fb",
              "IPY_MODEL_e9ea360c13f14354b534991d466a1f55"
            ],
            "layout": "IPY_MODEL_f52929d30df4490683f851d6125ba6cb"
          }
        },
        "963d34f7439f42a6be5449a3472744d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfe43cc23bfb4f0c98ca52183ad260c2",
            "placeholder": "​",
            "style": "IPY_MODEL_8ea3f3b1370e442ea12d2a7118799a58",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "e88335905554416b93dd90ac1e0f93fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8327272df32840aaabd4fec479a17a1f",
            "max": 2631639353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f63aff5c4a74e00bfc77f208e7dee2c",
            "value": 2631639353
          }
        },
        "e9ea360c13f14354b534991d466a1f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b1e311211b440c0a0de55d5c06199bf",
            "placeholder": "​",
            "style": "IPY_MODEL_6ba4638f89474a55885474c8d6359f17",
            "value": " 2.63G/2.63G [01:48&lt;00:00, 12.6MB/s]"
          }
        },
        "f52929d30df4490683f851d6125ba6cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfe43cc23bfb4f0c98ca52183ad260c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ea3f3b1370e442ea12d2a7118799a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8327272df32840aaabd4fec479a17a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f63aff5c4a74e00bfc77f208e7dee2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b1e311211b440c0a0de55d5c06199bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba4638f89474a55885474c8d6359f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c50dfed7a5047ef83e2300c88aed99b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8684163d63eb45e7ad956455a6ad2c8a",
              "IPY_MODEL_fcbe278955544f3d9aa04d08d1cf8d88",
              "IPY_MODEL_843d56b8232440dcb7e6e8a3b75f8fca"
            ],
            "layout": "IPY_MODEL_4edfd42be5a3442b8aafbdb654f0a99a"
          }
        },
        "8684163d63eb45e7ad956455a6ad2c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cd83e4d75ea4ae2a534f753daef63ff",
            "placeholder": "​",
            "style": "IPY_MODEL_d179b0f76b3c462885420195290b4eb6",
            "value": "generation_config.json: 100%"
          }
        },
        "fcbe278955544f3d9aa04d08d1cf8d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97d870e883e043338c3062aae764a65e",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e26c6b5eaec45e6989400d907c82535",
            "value": 137
          }
        },
        "843d56b8232440dcb7e6e8a3b75f8fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_923de711ca6e4329857a0ac7d14cf67b",
            "placeholder": "​",
            "style": "IPY_MODEL_b46a3e50d21b4e21ae90b61a46fece84",
            "value": " 137/137 [00:00&lt;00:00, 9.30kB/s]"
          }
        },
        "4edfd42be5a3442b8aafbdb654f0a99a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd83e4d75ea4ae2a534f753daef63ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d179b0f76b3c462885420195290b4eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97d870e883e043338c3062aae764a65e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e26c6b5eaec45e6989400d907c82535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "923de711ca6e4329857a0ac7d14cf67b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46a3e50d21b4e21ae90b61a46fece84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}